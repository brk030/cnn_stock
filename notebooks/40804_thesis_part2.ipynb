{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle \n",
    "import itertools\n",
    "import os \n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curation_download(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    df = df[[\"Exchange Date\", \"Close\", \"Open\", \"Low\", \"High\", \"Volume\"]][::-1]\n",
    "    df[\"Exchange Date\"] = pd.to_datetime(df[\"Exchange Date\"])#.dt.floor(\"d\")\n",
    "    df = df.set_index(\"Exchange Date\")\n",
    "    df.index.names = [\"Date\"]\n",
    "    df = df.astype(np.float64)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def technical_indicators(data, column):\n",
    "    \n",
    "    df = data.copy()\n",
    "    #SMA\n",
    "    df[\"SMA_10\"] = df[column].rolling(window = 10).mean()\n",
    "    df[\"SMA_50\"] = df[column].rolling(window = 50).mean()\n",
    "\n",
    "    # EMA\n",
    "    df[\"EMA_10\"] = df[column].ewm(span = 50, adjust = False).mean()\n",
    "    df[\"EMA_50\"] = df[column].ewm(span = 50, adjust = False).mean() # look at adjust\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df[\"SMA_20\"] = df[column].rolling(window = 20).mean()\n",
    "    df[\"upper_band\"] = df[\"SMA_20\"] + 2 * df[column].rolling(window = 20).std()\n",
    "    df[\"lower_band\"] = df[\"SMA_20\"] - 2 * df[column].rolling(window = 20).std()\n",
    "\n",
    "    # MACD\n",
    "    df[\"EMA_12\"] = df[column].ewm(span = 12, adjust = False).mean()\n",
    "    df[\"EMA_26\"] = df[column].ewm(span = 26, adjust = False).mean() #might lead to leakage\n",
    "    df[\"MACD\"] = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
    "    df[\"Signal_Line\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    return df, df.columns\n",
    "\n",
    "\n",
    "def additional_features(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    features = pd.DataFrame(index = df.index)\n",
    "    \n",
    "    # zscore\n",
    "    features[\"f01\"] = df[\"High\"] - df[\"Close\"] # upper shadow\n",
    "    features[\"f02\"] = df[\"Open\"] - df[\"Low\"] # lower shadow\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check VW, BMW\n",
    "all_stocks = []\n",
    "stock_names = [\"Daimler\", \"Bayer\", \"SAP\", \"Deutsche Bank\"]\n",
    "\n",
    "daimler = pd.read_excel(\"../data/Daimler(USD)/Price History.xlsx\", skiprows = 32, usecols = \"A:I\")\n",
    "daimler = curation_download(daimler)\n",
    "all_stocks.append(daimler)\n",
    "\n",
    "bayer = pd.read_excel(\"../data/Bayer(USD)/Price History.xlsx\", skiprows = 30, usecols = \"A:I\")\n",
    "bayer = curation_download(bayer)\n",
    "all_stocks.append(bayer)\n",
    "\n",
    "sap = pd.read_excel(\"../data/SAP(USD)/Price History.xlsx\", skiprows = 31, usecols = \"A:I\")\n",
    "sap = curation_download(sap)\n",
    "all_stocks.append(sap)\n",
    "\n",
    "db = pd.read_excel(\"../data/DB(USD)/Price History.xlsx\", skiprows = 29, usecols = \"A:I\")\n",
    "db = curation_download(db)\n",
    "all_stocks.append(db)\n",
    "\n",
    "# economic data\n",
    "comex_gold = pd.read_excel(\"../data/Comex(Gold)/Price History.xlsx\", skiprows = 31, usecols = \"A:I\")\n",
    "comex_gold = comex_gold[[\"Exchange Date\", \"Close\", \"Volume\"]][::-1]\n",
    "comex_gold[\"Exchange Date\"] = pd.to_datetime(comex_gold[\"Exchange Date\"])#.dt.floor(\"d\")\n",
    "comex_gold = comex_gold.set_index(\"Exchange Date\")\n",
    "comex_gold.index.names = [\"Date\"]\n",
    "comex_gold = comex_gold.astype(np.float64)\n",
    "\n",
    "eur_usd = pd.read_excel(\"../data/EUR-USD/Price History.xlsx\", skiprows = 26, usecols = \"A:H\")\n",
    "eur_usd = eur_usd[[\"Exchange Date\", \"Bid\", \"Ask\", \"High\", \"Low\", \"Open\"]]\n",
    "eur_usd[\"Exchange Date\"] = pd.to_datetime(eur_usd[\"Exchange Date\"])\n",
    "eur_usd = eur_usd.set_index(\"Exchange Date\")\n",
    "eur_usd.index.names = [\"Date\"]\n",
    "eur_usd = eur_usd.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_df = []\n",
    "technical_cols = []\n",
    "\n",
    "for i in range(len(all_stocks)):\n",
    "    technical, tech_cols = technical_indicators(all_stocks[i], \"Close\")\n",
    "    technical_df.append(technical)\n",
    "    technical_cols.append(tech_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(technical_df)):\n",
    "    display(technical_df[i].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feat = []\n",
    "\n",
    "for i in range(len(all_stocks)):\n",
    "    add_feat.append(additional_features(all_stocks[i]))\n",
    "    \n",
    "for i in range(len(add_feat)):\n",
    "    display(add_feat[i].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge everything including economic data\n",
    "final_df = []\n",
    "for i in range(len(all_stocks)):\n",
    "    feat_df = pd.merge(technical_df[i], add_feat[i], on = \"Date\")\n",
    "    feat_df = pd.merge(feat_df, comex_gold[\"Close\"], on = \"Date\", suffixes = [None, \"_gold\"])\n",
    "    feat_df = pd.merge(feat_df , eur_usd[[\"High\", \"Low\", \"Open\"]], on = \"Date\", suffixes = [None, \"_fx\"])\n",
    "    #feat_df = feat_df.drop([\"High\", \"Low\", \"Open\"], axis = 1)\n",
    "    feat_df = feat_df.dropna()\n",
    "    final_df.append(feat_df)\n",
    "    \n",
    "for i in range(len(final_df)):\n",
    "    display(final_df[i].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix (annotation True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_df)):\n",
    "    \n",
    "    corrMatrix = final_df[i].corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20, 20))\n",
    "    sns.heatmap(corrMatrix, annot = True, ax = ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "\n",
    "for i in range(len(final_df)):\n",
    "    \n",
    "    drop_matrix = final_df[i].corr().abs()\n",
    "    upper = drop_matrix.where(np.triu(np.ones(drop_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
    "    final_df[i].drop(to_drop, axis = 1, inplace = True)\n",
    "    final_data.append(final_df[i])\n",
    "    \n",
    "for i in range(len(final_data)):\n",
    "    \n",
    "    print(final_data[i].shape)\n",
    "    display(final_data[i].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check order\n",
    "for i in range(len(final_data)):\n",
    "    print(stock_names[i])\n",
    "    print(final_data[i].index[0])\n",
    "    print(final_data[i].index[-1])\n",
    "    print(final_data[i].shape)\n",
    "    print(\"*\"*35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create labels and plot correlation/ label - feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_vol(df, span=20):\n",
    "    \n",
    "    daily_returns = df.pct_change()\n",
    "    \n",
    "    return daily_returns.ewm(span=span).std()\n",
    "\n",
    "\n",
    "def create_labels(df, daily_vol, t_final = 10, upper_lower_multipliers = [1, 1]):\n",
    "\n",
    "    out = pd.DataFrame(index = daily_vol.index, columns = [\"date_passed\", \"label\", \"initial\", \"upper\", \n",
    "                                                           \"lower\", \"break\", \"final\"])\n",
    "    \n",
    "    for day, vol in daily_vol.iterrows():\n",
    "        days_passed = len(daily_vol.loc[daily_vol.index[0] : day])\n",
    "\n",
    "        if (days_passed + t_final < len(daily_vol.index) and t_final != 0):\n",
    "            vert_barrier = daily_vol.index[days_passed + t_final]\n",
    "\n",
    "        else:\n",
    "            vert_barrier = np.nan\n",
    "\n",
    "        if upper_lower_multipliers[0] > 0:\n",
    "            top_barrier = df[\"Close\"][day] + df[\"Close\"][day] * upper_lower_multipliers[0] * vol\n",
    "        else:\n",
    "            top_barrier = pd.Series(index=close_prices.index)\n",
    "\n",
    "        if upper_lower_multipliers[1] > 0:\n",
    "            bot_barrier = df[\"Close\"][day] - df[\"Close\"][day] * upper_lower_multipliers[1] * vol\n",
    "        else:\n",
    "            bot_barrier = pd.Series(index=close_prices.index)\n",
    "\n",
    "        breakthrough_date = vert_barrier\n",
    "        out.at[day, \"initial\"] = df[\"Close\"][day]\n",
    "        out.at[day, \"upper\"] = top_barrier[\"Close\"]\n",
    "        out.at[day, \"lower\"] = bot_barrier[\"Close\"]\n",
    "        out.at[day, \"break\"] = breakthrough_date\n",
    "\n",
    "        for future_date in daily_vol.index[days_passed : min(days_passed + t_final, len(daily_vol.index))]:\n",
    "            if ((df[\"Close\"].loc[future_date] >= top_barrier[\"Close\"] and top_barrier[\"Close\"] != 0)):\n",
    "                out.at[day, \"date_passed\"] = future_date\n",
    "                out.at[day, \"label\"] = 1 \n",
    "                out.at[day, \"final\"] = df[\"Close\"].loc[future_date]\n",
    "                breakthrough_date = future_date\n",
    "                break\n",
    "\n",
    "            elif ((df[\"Close\"].loc[future_date] <= bot_barrier[\"Close\"] and bot_barrier[\"Close\"] != 0)):\n",
    "                out.at[day, \"date_passed\"] = future_date\n",
    "                out.at[day, \"label\"] = -1\n",
    "                out.at[day, \"final\"] = df[\"Close\"].loc[future_date]\n",
    "                breakthrough_date = future_date\n",
    "                break\n",
    "\n",
    "        if (breakthrough_date == vert_barrier):\n",
    "            price_initial = df[\"Close\"].loc[day]\n",
    "            price_final = df[\"Close\"].loc[breakthrough_date]\n",
    "\n",
    "            if price_final > top_barrier[\"Close\"]:\n",
    "                out.at[day, \"date_passed\"] = \"break_through\"\n",
    "                out.at[day, \"label\"] = 1\n",
    "                out.at[day, \"final\"] = price_final\n",
    "\n",
    "            elif price_final < bot_barrier[\"Close\"]:\n",
    "                out.at[day, \"date_passed\"] = \"break_through\"\n",
    "                out.at[day, \"label\"] = -1\n",
    "                out.at[day, \"final\"] = price_final\n",
    "\n",
    "            else:\n",
    "                out.at[day, \"date_passed\"] = \"calculated\"\n",
    "                out.at[day, \"label\"] = max([(price_final - price_initial) / (top_barrier[\"Close\"] - price_initial),\n",
    "                                            (price_final - price_initial) / (price_initial - bot_barrier[\"Close\"])],\n",
    "                                           key=abs)\n",
    "                out.at[day, \"final\"] = price_final\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = []\n",
    "\n",
    "for i in range(len(final_data)):\n",
    "    vol = get_daily_vol(final_data[i][[\"Close\"]]).dropna()\n",
    "    labels = create_labels(final_data[i], vol, t_final = 10, upper_lower_multipliers = [2, 2])\n",
    "    labels = labels[:labels[\"label\"].isnull().values.argmax()]\n",
    "    labels[\"label\"] = np.where(labels[\"label\"] >= 0, 1, -1)\n",
    "    labels_df.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = []\n",
    "\n",
    "for i in range(len(final_data)):\n",
    "    final_df.append(final_data[i].join(labels_df[i][\"label\"], on = \"Date\").dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_df)):\n",
    "    \n",
    "    x = final_df[i].corr()[\"label\"].sort_values(ascending=False)[1:].copy()\n",
    "    y = x.index \n",
    "\n",
    "    fig = plt.figure(figsize = (10, 7))\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_title('Correlation Between Attributes and Target ({})'.format(stock_names[i]), fontsize=16)\n",
    "    ax.barh(np.arange(len(y)), x, align='center')\n",
    "    ax.set_yticks(np.arange(len(y)))\n",
    "    ax.set_yticklabels(y)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Correlation', fontsize=14)\n",
    "    ax.set_ylabel('Attributes', fontsize=14)\n",
    "    ax.axvline(linewidth=1, color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller \n",
    "\n",
    "def getWeights_FFD(d=0.1, thres=1e-5):\n",
    "    \n",
    "    w,k=[1.],1\n",
    "    while True:\n",
    "        w_=-w[-1]/k*(d-k+1)\n",
    "        if abs(w_)<thres:break\n",
    "        w.append(w_)\n",
    "        k+=1\n",
    "    return np.array(w[::-1]).reshape(-1,1)\n",
    "\n",
    "\n",
    "def transfer_data_by_frac_diff_FFD(col, d=0.1, thres=1e-4):\n",
    "    #3) Apply weights to values\n",
    "    w=getWeights_FFD(d,thres)\n",
    "    width=len(w)-1\n",
    "    \n",
    "    df = pd.Series()\n",
    "    #widow size can't be larger than the size of data\n",
    "    if width >= col.shape[0]:\n",
    "        raise Exception(\"width is oversize\")\n",
    "        \n",
    "    for i in range(width, col.shape[0]):\n",
    "        i_0_index, i_1_index = col.index[i-width], col.index[i]\n",
    "        data = np.dot(w.T, col.loc[i_0_index:i_1_index])[0]\n",
    "        \n",
    "        df[i_1_index] = data\n",
    "                   \n",
    "    return df\n",
    "\n",
    "\n",
    "def MemoryVsCorr(total_range, series, treshold):\n",
    "    \n",
    "    interval = np.linspace(total_range[0], total_range[1], total_range[2])\n",
    "    result = pd.DataFrame(np.zeros((len(interval),4)))\n",
    "    result.columns = ['order','adf','corr', '5%']\n",
    "    result['order'] = interval\n",
    "\n",
    "    for counter,order in enumerate(interval):\n",
    "        seq_traf = transfer_data_by_frac_diff_FFD(series, order, treshold)\n",
    "        res=adfuller(seq_traf, maxlag=1, regression='c') #autolag='AIC'\n",
    "        result.loc[counter,'adf']=res[0]\n",
    "        result.loc[counter,'5%']=res[4]['5%']\n",
    "        seq_traf = seq_traf.values\n",
    "        difference = len(series) - len(seq_traf)\n",
    "        df1 = series[difference:].reset_index(drop=True).values\n",
    "        result.loc[counter, 'corr'] = np.corrcoef(df1, seq_traf)[0,1]\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def df_after_frac(df_before, df_after):\n",
    "\n",
    "    difference = len(df_before) - len(df_after)\n",
    "    df_frac = df_after.to_frame().reset_index().copy()\n",
    "    df_frac.columns = [\"Date\", \"Diff\"]\n",
    "    df_frac.set_index(\"Date\", inplace = True)\n",
    "    \n",
    "    df_final = df_before[[\"Close\", \"Volume\", \"MACD\", \"Signal_Line\"]].merge(df_frac, on='Date', how='left')[difference:]\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_df)):\n",
    "    final_df[i].drop(\"label\", axis = 1, inplace = True)\n",
    "    display(final_df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stock_num = 0\n",
    "adf_list = []\n",
    "while stock_num < (len(stock_names)):\n",
    "    df_adf = pd.DataFrame(columns=[final_df[stock_num].columns])\n",
    "    print(stock_names[stock_num])\n",
    "    values_df = []\n",
    "    for col in final_df[stock_num].columns:\n",
    "        test_adf = MemoryVsCorr([0, 1, 21], final_df[stock_num][col], 1e-3)\n",
    "        for i in range(len(test_adf)):\n",
    "            if test_adf.loc[i][\"adf\"] < test_adf.loc[i][\"5%\"]:\n",
    "                values_df.append(test_adf.loc[i][\"order\"])\n",
    "                break\n",
    "            elif i == (len(test_adf)-1):\n",
    "                values_df.append(\"out of range\")\n",
    "            else:\n",
    "                pass\n",
    "    df_adf.loc[len(df_adf)] = values_df\n",
    "    adf_list.append(df_adf)\n",
    "    stock_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(adf_list)):\n",
    "    print(stock_names[i])\n",
    "    display(adf_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frac = []\n",
    "\n",
    "for i, data in enumerate(final_df):\n",
    "    df = pd.DataFrame(index = data.index)\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if adf_list[i].loc[0][col].item() != 0:\n",
    "            print(stock_names[i])\n",
    "            print(col)\n",
    "            display(adf_list[i].loc[0])\n",
    "            fractioned = transfer_data_by_frac_diff_FFD(data[col], adf_list[i].loc[0][col].item(),\n",
    "                                                        thres = 1e-3)\n",
    "            \n",
    "            fractioned = fractioned.to_frame().reset_index()\n",
    "            fractioned.columns = [\"Date\", \"diff_{}\".format(col)]\n",
    "            fractioned.set_index(\"Date\", inplace=True)\n",
    "            display(fractioned)\n",
    "            df = df.join(fractioned, on = \"Date\")\n",
    "            print(\"-\" * 35)\n",
    "        else:\n",
    "            df = df.join(data[[col]])\n",
    "            \n",
    "    df = df.dropna()    \n",
    "    df_frac.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = open(\"../data/adf_data_new.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(stock_names, file)\n",
    "pickle.dump(labels_df, file)\n",
    "pickle.dump(final_df, file)\n",
    "pickle.dump(adf_list, file)\n",
    "pickle.dump(df_frac, file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data/adf_data_new.pkl\", \"rb\")\n",
    "\n",
    "stock_names = pickle.load(file)\n",
    "labels_df = pickle.load(file)\n",
    "final_df = pickle.load(file)\n",
    "adf_list = pickle.load(file)\n",
    "df_frac = pickle.load(file)\n",
    "\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels_df)):\n",
    "    print(stock_names[i])\n",
    "    display(labels_df[i].head(1))\n",
    "    display(final_df[i].head(1))\n",
    "    display(df_frac[i].head(1))\n",
    "    print(\"*\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = []\n",
    "\n",
    "for i in range(len(df_frac)):\n",
    "    result = pd.merge(df_frac[i], labels_df[i][\"label\"], on = \"Date\", how = \"left\")\n",
    "    final_df.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = []\n",
    "cnn_labels = []\n",
    "\n",
    "for i in range(len(final_df)):\n",
    "    cnn_labels.append(final_df[i][[\"label\"]])\n",
    "    data = final_df[i].drop(\"label\", axis = 1)\n",
    "    print(len(cnn_labels[i]) == len(data))\n",
    "    cnn_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.image import GramianAngularField\n",
    "import math\n",
    "\n",
    "def create_data(df, df_label, stock_names):\n",
    "    \n",
    "    df_new = ((df - df.max()) + (df - df.min()))/(df.max() - df.min())\n",
    "    df_scaled = df_new.copy()\n",
    "    for method in [\"summation\", \"difference\"]:\n",
    "        print(stock_names, method)\n",
    "        X_gasf_list = []\n",
    "        train_dax = []\n",
    "        test_dax = []\n",
    "        target_train = []\n",
    "        target_test = []\n",
    "        \n",
    "        for i in range(30, len(df_scaled), 1):\n",
    "            X = df_scaled.iloc[i-30:i].T.copy()\n",
    "            gasf = GramianAngularField(method = method)\n",
    "            X_gasf = gasf.fit_transform(X)\n",
    "            X_gasf_list = [i for i in X_gasf]\n",
    "            dimensional = np.dstack(X_gasf_list)\n",
    "\n",
    "            target = df_label[\"label\"].iloc[i-1]\n",
    "            \n",
    "            assert df_label.iloc[i-1].name == X.columns[-1]\n",
    "            \n",
    "            train_dax.append(dimensional)\n",
    "            target_train.append(target)\n",
    "\n",
    "\n",
    "        path_train = \"../eikon_api/{}/{}/Train\".format(method, stock_names)\n",
    "\n",
    "        os.makedirs(path_train)\n",
    "\n",
    "        np.save(os.path.join(\"../eikon_api/{}/{}\".format(method, stock_names), \"Train\", \"train.npy\"), train_dax)\n",
    "\n",
    "        np.savetxt(os.path.join(\"../eikon_api/{}/{}\".format(method, stock_names), \"Train\", \"target_train.csv\"), \n",
    "                   target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cnn_data)):\n",
    "    create_data(cnn_data[i], cnn_labels[i], stock_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum = []\n",
    "train_diff = []\n",
    "train_label = []\n",
    "\n",
    "for method in [\"summation\", \"difference\"]:\n",
    "    for i in range(len(stock_names)):\n",
    "        \n",
    "        if method == \"summation\":\n",
    "            train_sum.append(np.load(os.path.join(\"../eikon_api/{}/{}\".format(method, stock_names[i]), \n",
    "                                              \"Train\", \"train.npy\")))\n",
    "            train_label.append(pd.read_csv(\"../eikon_api/{}/{}/Train/target_train.csv\".format(method, stock_names[i]), \n",
    "                                           header = None))\n",
    "            \n",
    "        if method == \"difference\":\n",
    "            train_diff.append(np.load(os.path.join(\"../eikon_api/{}/{}\".format(method, \n",
    "                                                                          stock_names[i]), \"Train\", \"train.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length Train Sum: \", len(train_sum))\n",
    "print(\"Shape Train Sum: \", train_sum[0].shape)\n",
    "print(\"*\" * 40)\n",
    "print(\"Length Train Diff: \", len(train_diff))\n",
    "print(\"Shape Train Diff: \", train_diff[0].shape)\n",
    "print(\"*\" * 40)\n",
    "print(\"Length Train: \", len(train_label))\n",
    "print(\"Shape Train: \", train_label[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(len(train_sum[0][0,0,0,:])):\n",
    "    plt.subplot(4, 3, i+1)   \n",
    "    plt.axis('off')\n",
    "    plt.title(cnn_data[0].columns[i])\n",
    "    plt.imshow(train_sum[0][0,:,:,i], cmap = \"gray\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../data/plots/gaf-sum.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(len(train_diff[0][0,0,0,:])):\n",
    "    plt.subplot(4, 3, i+1)   \n",
    "    plt.axis('off')\n",
    "    plt.title(cnn_data[0].columns[i])\n",
    "    plt.imshow(train_diff[0][0,:,:,i], cmap = \"gray\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../data/plots/gaf-diff.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list = []\n",
    "\n",
    "for i in range(len(train_label)):\n",
    "    y_train = train_label[i].iloc[:,0].values\n",
    "    y_train = np.where(y_train == -1, 0, y_train)\n",
    "    y_train_list.append(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train_list)):\n",
    "    print(\"{}: {}\".format(stock_names[i], len(y_train_list[i])))\n",
    "    unique, counts = np.unique(y_train_list[i], return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(\"Share of {}: {}\".format(unique[0], round(counts[0]/sum(counts), 2)))\n",
    "    print(\"Share of {}: {}\".format(unique[1], round(counts[1]/sum(counts), 2)))\n",
    "    print(\"*\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def cnn_model(shape, pool_type = \"max\", conv_activation = \"relu\",batch_norm = True, drop_out_layer = 0.2, \n",
    "              neurons = 128, lr = 0.00001):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), input_shape = shape, \n",
    "                                     activation = conv_activation))    \n",
    "    if batch_norm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == None:\n",
    "        pass\n",
    "\n",
    "        \n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation = conv_activation))\n",
    "    if batch_norm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == None:\n",
    "        pass\n",
    "\n",
    "#     model.add(tf.keras.layers.Conv2D(128, (3, 3), activation = conv_activation))\n",
    "#     if batch_norm:\n",
    "#         model.add(tf.keras.layers.BatchNormalization())\n",
    "#     if pool_type == \"max\":\n",
    "#         model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#     if pool_type == \"average\":\n",
    "#         model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "#     if pool_type == None:\n",
    "#         pass\n",
    "\n",
    "        \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = neurons, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = neurons, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = 2, activation =  \"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "                  loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays as output otherwise will not work with sk-GS\n",
    "from sklearn.utils import indexable\n",
    "from sklearn.utils.validation import _num_samples\n",
    "\n",
    "class WalkingForward():\n",
    "    \n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None, train_splits = 2, test_splits = 1):\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        train_splits, test_splits = int(train_splits), int(test_splits)\n",
    "        indices = np.arange(n_samples)\n",
    "        split_size = (n_samples // n_folds)\n",
    "        test_size = split_size * test_splits\n",
    "        train_size = split_size * train_splits\n",
    "        test_starts = range(train_size + n_samples % n_folds, n_samples - (test_size - split_size), split_size)\n",
    "        for i, test_start in zip(range(len(test_starts)), test_starts):\n",
    "            rem = 0\n",
    "            if i == 0:\n",
    "                rem = n_samples % n_folds\n",
    "            yield (indices[(test_start - train_size - rem):test_start], indices[test_start:test_start + test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "wfcv = WalkingForward(n_splits = 11)\n",
    "\n",
    "for method in [\"summation\", \"difference\"]:\n",
    "    for num in range(4, 7, 1):\n",
    "        for i in range(len(train_sum)):\n",
    "\n",
    "            selection_df = pd.DataFrame()\n",
    "            \n",
    "            combinations_list = []\n",
    "            acc_list = []\n",
    "            auc_list = []\n",
    "            f1_list = []\n",
    "            precision_list = []\n",
    "            recall_list = []\n",
    "            y_pred_list = []\n",
    "            y_true_list = []\n",
    "            \n",
    "            if method == \"summation\":\n",
    "                print(stock_names[i], method, num)\n",
    "                train = train_sum[i].copy()\n",
    "                \n",
    "            elif method == \"difference\":\n",
    "                print(stock_names[i], method, num)\n",
    "                train = train_diff[i].copy()\n",
    "                \n",
    "            else:\n",
    "                print(\"None of the methods was found!\")\n",
    "                break\n",
    "                \n",
    "            y_train = y_train_list[i].copy()\n",
    "\n",
    "            assert len(cnn_data[i].columns) == train.shape[3]\n",
    "            all_combinations = list(itertools.combinations(range(len(cnn_data[i].columns)), num))\n",
    "            \n",
    "            print(\"Number of Combinations with {} features: \".format(num), len(all_combinations))\n",
    "            \n",
    "            for j in range(len(all_combinations)+1):\n",
    "                if j == 0:\n",
    "                    print(\"Base Line\")\n",
    "                    combinations_list.append([\"Base Line\"])\n",
    "                    X_train = train.copy()\n",
    "                else:\n",
    "                    combo = list(all_combinations[j-1])\n",
    "                    combo_cols = [cnn_data[i].columns[idx] for idx in combo]\n",
    "                    combinations_list.append(combo_cols)\n",
    "                    assert len(combo_cols) == num\n",
    "                    X_train = np.stack([train[:, :, :, x] for x in combo], axis = 3)\n",
    "\n",
    "                  \n",
    "                class_list = []\n",
    "                real_class_list = []\n",
    "                \n",
    "                input_shape = X_train.shape[1:]\n",
    "                tf.keras.backend.clear_session()\n",
    "                model = cnn_model(shape = input_shape)\n",
    "                \n",
    "                for train_idx, test_idx in wfcv.split(X_train):\n",
    "                    \n",
    "                    history = model.fit(X_train[train_idx,:,:,:], y_train[train_idx],\n",
    "                                        validation_data=(X_train[test_idx,:,:,:], y_train[test_idx]), epochs = 5, \n",
    "                                        verbose = 1, batch_size = 16)\n",
    "                    \n",
    "                    predict_class = model.predict_classes(X_train[test_idx,:,:,:])\n",
    "                    class_list.append(list(predict_class))\n",
    "                    real_class_list.append(y_train[test_idx])\n",
    "                    \n",
    "                y_pred_ = [item for sublist in class_list for item in sublist]\n",
    "                y_test_ = [item for sublist in real_class_list for item in sublist]\n",
    "                \n",
    "                acc_list.append(accuracy_score(y_test_, y_pred_))\n",
    "                auc_list.append(roc_auc_score(y_test_, y_pred_))\n",
    "                f1_list.append(f1_score(y_test_, y_pred_))\n",
    "                precision_list.append(precision_score(y_test_, y_pred_))\n",
    "                recall_list.append(recall_score(y_test_, y_pred_))\n",
    "                y_pred_list.append(y_pred_)\n",
    "                y_true_list.append(y_test_)\n",
    "            selection_df[\"{}_{}\".format(num, method)] = combinations_list\n",
    "            selection_df[\"accuracy\"] = acc_list\n",
    "            selection_df[\"auc\"] = auc_list\n",
    "            selection_df[\"f1\"] = f1_list\n",
    "            selection_df[\"precision\"] = precision_list\n",
    "            selection_df[\"recall\"] = recall_list\n",
    "            selection_df[\"y_pred\"] = y_pred_list\n",
    "            selection_df[\"y_true\"] = y_true_list\n",
    "            display(selection_df)\n",
    "            selection_df.to_csv(\"../data/scores/score{}_{}_{}.csv\".format(num, method, stock_names[i]), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def score_curation(stock_name):\n",
    "    files_list = [f for f in os.listdir(\"../data/scores\") if os.path.isfile(os.path.join(\"../data/scores\", f))]\n",
    "    stock_files = [f for f in files_list if stock_name in f]\n",
    "    summation_files = [method for method in stock_files if \"summation\" in method]\n",
    "    difference_files = [method for method in stock_files if \"difference\" in method]\n",
    "    \n",
    "    df_summation = []\n",
    "    df_difference = []\n",
    "    \n",
    "    for i in range(len(summation_files)):\n",
    "        \n",
    "        data_sum = pd.read_csv(\"../data/scores/{}\".format(summation_files[i]))\n",
    "        data_diff = pd.read_csv(\"../data/scores/{}\".format(difference_files[i]))\n",
    "        data_sum.columns = [\"features\", \"accuracy\", \"auc\", \"f1\", \"precision\", \"recall\", \"y_pred\", \"y_true\"]\n",
    "        data_diff.columns = [\"features\", \"accuracy\", \"auc\", \"f1\", \"precision\", \"recall\", \"y_pred\", \"y_true\"]\n",
    "        \n",
    "        data_sum[\"features\"] = data_sum.loc[:, \"features\"].apply(lambda x: literal_eval(x))\n",
    "        data_diff[\"features\"] = data_diff.loc[:, \"features\"].apply(lambda x: literal_eval(x))\n",
    "        data_sum[\"y_pred\"] = data_sum.loc[:, \"y_pred\"].apply(lambda x: literal_eval(x))\n",
    "        data_diff[\"y_pred\"] = data_diff.loc[:, \"y_pred\"].apply(lambda x: literal_eval(x))\n",
    "        data_sum[\"y_true\"] = data_sum.loc[:, \"y_true\"].apply(lambda x: literal_eval(x))\n",
    "        data_diff[\"y_true\"] = data_diff.loc[:, \"y_true\"].apply(lambda x: literal_eval(x))\n",
    "        \n",
    "        data_sum[\"method\"] = \"sum\"\n",
    "        data_diff[\"method\"] = \"diff\"\n",
    "        \n",
    "        df_summation.append(data_sum)\n",
    "        df_difference.append(data_diff)\n",
    "    \n",
    "    \n",
    "    final_sum = pd.concat(df_summation, ignore_index = True)\n",
    "    final_diff = pd.concat(df_difference, ignore_index = True)\n",
    "    \n",
    "    final = pd.concat([final_sum, final_diff], ignore_index = True)\n",
    "    final[\"len\"] = final[\"features\"].apply(lambda x: len(x))\n",
    "    final[\"features\"] = [','.join(map(str, string)) for string in final['features']]\n",
    "    \n",
    "#     final = final.groupby([\"features\", \"method\", \"len\"])[[\"accuracy\", \"auc\", \"f1\", \"precision\", \n",
    "#                                                           \"recall\", \"y_pred\", \"y_true\"]].mean().reset_index()    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daimler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daimler_scores = score_curation(\"Daimler\")\n",
    "\n",
    "display(daimler_scores[daimler_scores[\"features\"] == \"Base Line\"].groupby([\"features\", \"method\", \n",
    "                                                                   \"len\"])[[\"accuracy\", \"auc\", \"f1\", \"precision\", \n",
    "                                                                            \"recall\"]].mean().reset_index())\n",
    "\n",
    "daimler_scores.sort_values(by = \"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_daimler = [\"Volume\", \"Signal_Line\", \"f01\", \"diff_Close_gold\"]\n",
    "features_daimler_idx = [i for i in range(len(cnn_data[0].columns)) if list(cnn_data[0].columns)[i]\n",
    "                        in features_daimler]\n",
    "gs_daimler = np.stack([train_sum[0][:,:,:,i] for i in features_daimler_idx], axis = 3)\n",
    "\n",
    "print(gs_daimler.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayer_scores = score_curation(\"Bayer\")\n",
    "display(bayer_scores[bayer_scores[\"features\"] == \"Base Line\"].groupby([\"features\", \"method\", \n",
    "                                                                   \"len\"])[[\"accuracy\", \"auc\", \"f1\", \"precision\", \n",
    "                                                                            \"recall\"]].mean().reset_index())\n",
    "\n",
    "bayer_scores.sort_values(by = \"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bayer = [\"diff_Close\", \"Volume\", \"MACD\", \"Signal_Line\", \"f01\", \"f02\"]\n",
    "features_bayer_idx = [i for i in range(len(cnn_data[1].columns)) if list(cnn_data[1].columns)[i]\n",
    "                        in features_bayer]\n",
    "gs_bayer = np.stack([train_diff[1][:,:,:,i] for i in features_bayer_idx], axis = 3)\n",
    "\n",
    "print(gs_bayer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sap_scores = score_curation(\"SAP\")\n",
    "display(sap_scores[sap_scores[\"features\"] == \"Base Line\"].groupby([\"features\", \"method\", \n",
    "                                                                   \"len\"])[[\"accuracy\", \"auc\", \"f1\", \"precision\", \n",
    "                                                                            \"recall\"]].mean().reset_index())\n",
    "sap_scores.sort_values(by = \"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sap = [\"diff_Close\", \"MACD\", \"f01\", \"diff_Close_gold\", \"diff_High_fx\"]\n",
    "features_sap_idx = [i for i in range(len(cnn_data[2].columns)) if list(cnn_data[2].columns)[i]\n",
    "                        in features_sap]\n",
    "gs_sap = np.stack([train_sum[2][:,:,:,i] for i in features_sap_idx], axis = 3)\n",
    "\n",
    "print(gs_sap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deutsche Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_scores = score_curation(\"Deutsche Bank\")\n",
    "\n",
    "display(db_scores[db_scores[\"features\"] == \"Base Line\"].groupby([\"features\", \"method\", \n",
    "                                                                   \"len\"])[[\"accuracy\", \"auc\", \"f1\", \"precision\", \n",
    "                                                                            \"recall\"]].mean().reset_index())\n",
    "\n",
    "db_scores.sort_values(by = \"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_db = [\"diff_Close\", \"MACD\", \"diff_Close_gold\", \"diff_High_fx\"]\n",
    "features_db_idx = [i for i in range(len(cnn_data[3].columns)) if list(cnn_data[3].columns)[i]\n",
    "                        in features_db]\n",
    "gs_db = np.stack([train_sum[3][:,:,:,i] for i in features_db_idx], axis = 3)\n",
    "\n",
    "print(gs_db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_data = [gs_daimler, gs_bayer, gs_sap, gs_db]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final model/ hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Daimler shape: \", gs_daimler.shape)\n",
    "print(\"Bayer shape: \", gs_bayer.shape)\n",
    "print(\"SAP shape: \", gs_sap.shape)\n",
    "print(\"Deutsche Bank: \", gs_db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement own random search\n",
    "# add none to pooling layer\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"pool_type\":[\"max\", \"average\"],\n",
    "    \"conv_activation\":[\"relu\", \"sigmoid\", \"tanh\"],\n",
    "    \"lr\":[0.001, 0.0001],\n",
    "    \"batch_norm\":[True, False],\n",
    "    \"drop_out_layer\":[0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"batch_size\":[1, 16, 32, 64],\n",
    "    \"neurons\" : [32, 64, 128, 256],\n",
    "    \"epochs\":[10, 20, 30, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = 1\n",
    "for x in param_grid.values():\n",
    "    com *= len(x)\n",
    "print('There are {} combinations'.format(com))\n",
    "print('This would take {:.0f} days to finish.'.format((100 * com) / (60 * 60 * 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "wfcv = WalkingForward(n_splits = 11)\n",
    "\n",
    "random_results_df = []\n",
    "score_df_list = []\n",
    "\n",
    "\n",
    "for i, model_data in enumerate(gs_data):\n",
    "    print(stock_names[i])\n",
    "    shape = model_data.shape[1:]\n",
    "    y_train = y_train_list[i].copy()\n",
    "    \n",
    "    random_results = pd.DataFrame(columns = param_grid.keys())\n",
    "    score_df = pd.DataFrame()\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    f1_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    \n",
    "    for j in range(60):\n",
    "        is_in = True\n",
    "        while is_in == True:\n",
    "            params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "            is_in = (random_results == list(params.values())).all(1).any()\n",
    "            \n",
    "        if is_in == False:\n",
    "            print(params)\n",
    "            random_results.loc[len(random_results)] = list(params.values())\n",
    "            tf.keras.backend.clear_session()\n",
    "            model_cv = cnn_model(shape = shape, pool_type = params[\"pool_type\"], \n",
    "                                 conv_activation = params[\"conv_activation\"], batch_norm = params[\"batch_norm\"], \n",
    "                                 drop_out_layer = params[\"drop_out_layer\"], neurons = params[\"neurons\"], \n",
    "                                 lr = params[\"lr\"])\n",
    "            \n",
    "            class_list = []\n",
    "            real_class_list =  []\n",
    "            for train_idx, test_idx in wfcv.split(model_data):\n",
    "                \n",
    "                history = model_cv.fit(model_data[train_idx,:,:,:], y_train[train_idx],\n",
    "                                       validation_data=(model_data[test_idx,:,:,:], y_train[test_idx]),\n",
    "                                       epochs = params[\"epochs\"], verbose = 1, batch_size = params[\"batch_size\"])\n",
    "\n",
    "                predict_class = model_cv.predict_classes(model_data[test_idx,:,:,:])\n",
    "                class_list.append(list(predict_class))\n",
    "                real_class_list.append(y_train[test_idx])\n",
    "                print(predict_class)\n",
    "                \n",
    "            y_pred_ = [item for sublist in class_list for item in sublist]\n",
    "            y_test_ = [item for sublist in real_class_list for item in sublist]\n",
    "\n",
    "            acc_list.append(accuracy_score(y_test_, y_pred_))\n",
    "            auc_list.append(roc_auc_score(y_test_, y_pred_))\n",
    "            f1_list.append(f1_score(y_test_, y_pred_))\n",
    "            precision_list.append(precision_score(y_test_, y_pred_))\n",
    "            recall_list.append(recall_score(y_test_, y_pred_))\n",
    "            y_pred_list.append(y_pred_)\n",
    "            y_true_list.append(y_test_)\n",
    "        \n",
    "    score_df[\"accuracy\"] = acc_list\n",
    "    score_df[\"auc\"] = auc_list\n",
    "    score_df[\"f1\"] = f1_list\n",
    "    score_df[\"precision\"] = precision_list\n",
    "    score_df[\"recall\"] = recall_list\n",
    "    score_df[\"y_pred\"] = y_pred_list\n",
    "    score_df[\"y_true\"] = y_true_list\n",
    "    random_results_df.append(random_results)\n",
    "    score_df_list.append(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = open(\"../data/best_param_score_new.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(random_results_df, file)\n",
    "pickle.dump(score_df_list, file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in range(len(score_df_list)):\n",
    "    \n",
    "    print(\"Stock\", stock_names[i])\n",
    "\n",
    "    best_param_idx =score_df_list[i].sort_values(by = \"accuracy\", ascending = False).reset_index().loc[0][\"index\"]\n",
    "    display(score_df_list[i].sort_values(by = \"accuracy\", ascending = False).reset_index().loc[[0]])\n",
    "        \n",
    "    display(random_results_df[i].loc[[best_param_idx]])\n",
    "    print(\"True\", Counter(score_df_list[i].loc[best_param_idx][\"y_true\"]))\n",
    "\n",
    "    print(\"Zero Share\", max(list(Counter(score_df_list[i].loc[best_param_idx][\"y_true\"]).values())) / \n",
    "          sum(list(Counter(score_df_list[i].loc[best_param_idx][\"y_true\"]).values())))\n",
    "\n",
    "    print(\"Pred\", Counter(score_df_list[i].loc[best_param_idx][\"y_pred\"]))\n",
    "    print(\"*\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
