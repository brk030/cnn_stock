{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle \n",
    "import itertools\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curation_download(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    df = df[[\"Exchange Date\", \"Close\", \"Open\", \"Low\", \"High\", \"Volume\", \"Turnover - USD\"]][::-1]\n",
    "    df[\"Exchange Date\"] = pd.to_datetime(df[\"Exchange Date\"])#.dt.floor(\"d\")\n",
    "    df = df.set_index(\"Exchange Date\")\n",
    "    df.index.names = [\"Date\"]\n",
    "    df = df.astype(np.float64)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def technical_indicators(data, column):\n",
    "    \n",
    "    df = data.copy()\n",
    "    #SMA\n",
    "    df[\"SMA_10\"] = df[column].rolling(window = 10).mean()\n",
    "    df[\"SMA_50\"] = df[column].rolling(window = 50).mean()\n",
    "\n",
    "    # EMA\n",
    "    df[\"EMA_10\"] = df[column].ewm(span = 50, adjust = False).mean()\n",
    "    df[\"EMA_50\"] = df[column].ewm(span = 50, adjust = False).mean() # look at adjust\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df[\"SMA_20\"] = df[column].rolling(window = 20).mean()\n",
    "    df[\"upper_band\"] = df[\"SMA_20\"] + 2 * df[column].rolling(window = 20).std()\n",
    "    df[\"lower_band\"] = df[\"SMA_20\"] - 2 * df[column].rolling(window = 20).std()\n",
    "\n",
    "    # MACD\n",
    "    df[\"EMA_12\"] = df[column].ewm(span = 12, adjust = False).mean()\n",
    "    df[\"EMA_26\"] = df[column].ewm(span = 26, adjust = False).mean() #might lead to leakage\n",
    "    df[\"MACD\"] = df[\"EMA_12\"] - df[\"EMA_26\"]\n",
    "    df[\"Signal_Line\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    return df, df.columns\n",
    "\n",
    "\n",
    "def additional_features(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    features = pd.DataFrame(index = df.index)    \n",
    "    # zscore\n",
    "    features[\"f01\"] = df[\"Close\"].rolling(window=200, min_periods=20).mean() / df[\"Close\"].rolling(window=200, \n",
    "                                                                                                   min_periods=20).std()\n",
    "    features[\"f02\"] = df[\"High\"] - df[\"Close\"] # upper shadow\n",
    "    features[\"f03\"] = df[\"Open\"] - df[\"Low\"] # lower shadow\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check VW, BMW\n",
    "all_stocks = []\n",
    "stock_names = [\"Daimler\", \"Bayer\"]\n",
    "\n",
    "daimler = pd.read_excel(\"../data/Daimler(EUR)/Price History.xlsx\", skiprows = 32, usecols = \"A:J\")\n",
    "daimler = curation_download(daimler)\n",
    "all_stocks.append(daimler)\n",
    "\n",
    "bayer = pd.read_excel(\"../data/Bayer(EUR)/Price History.xlsx\", skiprows = 30, usecols = \"A:I\")\n",
    "bayer = curation_download(bayer)\n",
    "all_stocks.append(bayer)\n",
    "\n",
    "\n",
    "\n",
    "# macro-economic data\n",
    "comex_gold = pd.read_excel(\"../data/Comex(Gold)/Price History.xlsx\", skiprows = 30, usecols = \"A:I\")\n",
    "comex_gold = comex_gold[[\"Exchange Date\", \"Close\", \"Volume\"]][::-1]\n",
    "comex_gold[\"Exchange Date\"] = pd.to_datetime(comex_gold[\"Exchange Date\"])#.dt.floor(\"d\")\n",
    "comex_gold = comex_gold.set_index(\"Exchange Date\")\n",
    "comex_gold.index.names = [\"Date\"]\n",
    "comex_gold = comex_gold.astype(np.float64)\n",
    "\n",
    "eur_usd = pd.read_excel(\"../data/EUR-USD/Price History.xlsx\", skiprows = 26, usecols = \"A:H\")\n",
    "eur_usd = eur_usd[[\"Exchange Date\", \"Bid\", \"Ask\", \"High\", \"Low\", \"Open\"]]\n",
    "eur_usd[\"Exchange Date\"] = pd.to_datetime(eur_usd[\"Exchange Date\"])\n",
    "eur_usd = eur_usd.set_index(\"Exchange Date\")\n",
    "eur_usd.index.names = [\"Date\"]\n",
    "eur_usd = eur_usd.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_df = []\n",
    "technical_cols = []\n",
    "\n",
    "for i in range(len(all_stocks)):\n",
    "    technical, tech_cols = technical_indicators(all_stocks[i], \"Close\")\n",
    "    technical_df.append(technical)\n",
    "    technical_cols.append(tech_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_df[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feat = []\n",
    "\n",
    "for i in range(len(all_stocks)):\n",
    "    add_feat.append(additional_features(all_stocks[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feat[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge everything including macro economic data\n",
    "\n",
    "final_df = []\n",
    "\n",
    "for i in range(len(all_stocks)):\n",
    "    feat_df = pd.merge(technical_df[i], add_feat[i], on = \"Date\")\n",
    "    feat_df = pd.merge(feat_df, comex_gold[\"Close\"], on = \"Date\", suffixes = [None, \"_gold\"])\n",
    "    feat_df = pd.merge(feat_df , eur_usd[[\"High\", \"Low\", \"Open\"]], on = \"Date\", suffixes = [None, \"_fx\"])\n",
    "    feat_df = feat_df.drop([\"High\", \"Low\", \"Open\"], axis = 1)\n",
    "    feat_df = feat_df.dropna()\n",
    "    final_df.append(feat_df)\n",
    "    \n",
    "final_df[0].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix (annotation True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = final_df[0].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20, 20))\n",
    "sns.heatmap(corrMatrix, annot = True, ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "\n",
    "for i in range(len(final_df)):\n",
    "    \n",
    "    drop_matrix = final_df[i].corr().abs()\n",
    "    upper = drop_matrix.where(np.triu(np.ones(drop_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
    "    final_df[i].drop(to_drop, axis = 1, inplace = True)\n",
    "    final_data.append(final_df[i])\n",
    "    \n",
    "print(final_data[0].shape)\n",
    "final_data[0].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create labels and plot correlation/ label - feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_vol(df, span=20):\n",
    "    \n",
    "    daily_returns = df.pct_change()\n",
    "    \n",
    "    return daily_returns.ewm(span=span).std()\n",
    "\n",
    "\n",
    "def create_labels(df, daily_vol, t_final = 10, upper_lower_multipliers = [1, 1]):\n",
    "\n",
    "    out = pd.DataFrame(index = daily_vol.index, columns = [\"date_passed\", \"label\", \"initial\", \"upper\", \n",
    "                                                           \"lower\", \"break\", \"final\"])\n",
    "    \n",
    "    for day, vol in daily_vol.iterrows():\n",
    "        days_passed = len(daily_vol.loc[daily_vol.index[0] : day])\n",
    "\n",
    "        if (days_passed + t_final < len(daily_vol.index) and t_final != 0):\n",
    "            vert_barrier = daily_vol.index[days_passed + t_final]\n",
    "\n",
    "        else:\n",
    "            vert_barrier = np.nan\n",
    "\n",
    "        if upper_lower_multipliers[0] > 0:\n",
    "            top_barrier = df[\"Close\"][day] + df[\"Close\"][day] * upper_lower_multipliers[0] * vol\n",
    "        else:\n",
    "            top_barrier = pd.Series(index=close_prices.index)\n",
    "\n",
    "        if upper_lower_multipliers[1] > 0:\n",
    "            bot_barrier = df[\"Close\"][day] - df[\"Close\"][day] * upper_lower_multipliers[1] * vol\n",
    "        else:\n",
    "            bot_barrier = pd.Series(index=close_prices.index)\n",
    "\n",
    "        breakthrough_date = vert_barrier\n",
    "        out.at[day, \"initial\"] = df[\"Close\"][day]\n",
    "        out.at[day, \"upper\"] = top_barrier[\"Close\"]\n",
    "        out.at[day, \"lower\"] = bot_barrier[\"Close\"]\n",
    "        out.at[day, \"break\"] = breakthrough_date\n",
    "\n",
    "        for future_date in daily_vol.index[days_passed : min(days_passed + t_final, len(daily_vol.index))]:\n",
    "            if ((df[\"Close\"].loc[future_date] >= top_barrier[\"Close\"] and top_barrier[\"Close\"] != 0)):\n",
    "                out.at[day, \"date_passed\"] = future_date\n",
    "                out.at[day, \"label\"] = 1 \n",
    "                out.at[day, \"final\"] = df[\"Close\"].loc[future_date]\n",
    "                breakthrough_date = future_date\n",
    "                break\n",
    "\n",
    "            elif ((df[\"Close\"].loc[future_date] <= bot_barrier[\"Close\"] and bot_barrier[\"Close\"] != 0)):\n",
    "                out.at[day, \"date_passed\"] = future_date\n",
    "                out.at[day, \"label\"] = -1\n",
    "                out.at[day, \"final\"] = df[\"Close\"].loc[future_date]\n",
    "                breakthrough_date = future_date\n",
    "                break\n",
    "\n",
    "        if (breakthrough_date == vert_barrier):\n",
    "            price_initial = df[\"Close\"].loc[day]\n",
    "            price_final = df[\"Close\"].loc[breakthrough_date]\n",
    "\n",
    "            if price_final > top_barrier[\"Close\"]:\n",
    "                out.at[day, \"date_passed\"] = \"break_through\"\n",
    "                out.at[day, \"label\"] = 1\n",
    "                out.at[day, \"final\"] = price_final\n",
    "\n",
    "            elif price_final < bot_barrier[\"Close\"]:\n",
    "                out.at[day, \"date_passed\"] = \"break_through\"\n",
    "                out.at[day, \"label\"] = -1\n",
    "                out.at[day, \"final\"] = price_final\n",
    "\n",
    "            else:\n",
    "                out.at[day, \"date_passed\"] = \"calculated\"\n",
    "                out.at[day, \"label\"] = max([(price_final - price_initial) / (top_barrier[\"Close\"] - price_initial),\n",
    "                                            (price_final - price_initial) / (price_initial - bot_barrier[\"Close\"])],\n",
    "                                           key=abs)\n",
    "                out.at[day, \"final\"] = price_final\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = []\n",
    "\n",
    "for i in range(len(final_data)):\n",
    "    vol = get_daily_vol(final_data[i][[\"Close\"]]).dropna()\n",
    "    labels = create_labels(final_df[i], vol, t_final = 10, upper_lower_multipliers = [2, 2])\n",
    "    labels = labels[:labels[\"label\"].isnull().values.argmax()]\n",
    "    labels[\"label\"] = np.where(labels[\"label\"] > 0, 1, -1)\n",
    "    labels_df.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = []\n",
    "\n",
    "for i in range(len(final_data)):\n",
    "    final_df.append(final_data[i].join(labels_df[i][\"label\"], on = \"Date\").dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = final_df[1].corr()[\"label\"].sort_values(ascending=False)[1:]\n",
    "y = x.index \n",
    "\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title('Correlation Between Attributes and Target', fontsize=16)\n",
    "ax.barh(np.arange(len(y)), x, align='center')\n",
    "ax.set_yticks(np.arange(len(y)))\n",
    "ax.set_yticklabels(y)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Correlation', fontsize=14)\n",
    "ax.set_ylabel('Attributes', fontsize=14)\n",
    "ax.axvline(linewidth=1, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller \n",
    "\n",
    "def getWeights_FFD(d=0.1, thres=1e-5):\n",
    "    \n",
    "    w,k=[1.],1\n",
    "    while True:\n",
    "        w_=-w[-1]/k*(d-k+1)\n",
    "        if abs(w_)<thres:break\n",
    "        w.append(w_)\n",
    "        k+=1\n",
    "    return np.array(w[::-1]).reshape(-1,1)\n",
    "\n",
    "\n",
    "def transfer_data_by_frac_diff_FFD(col, d=0.1, thres=1e-4):\n",
    "    #3) Apply weights to values\n",
    "    w=getWeights_FFD(d,thres)\n",
    "    width=len(w)-1\n",
    "    \n",
    "    df = pd.Series()\n",
    "    #widow size can't be larger than the size of data\n",
    "    if width >= col.shape[0]:\n",
    "        raise Exception(\"width is oversize\")\n",
    "        \n",
    "    for i in range(width, col.shape[0]):\n",
    "        i_0_index, i_1_index = col.index[i-width], col.index[i]\n",
    "        data = np.dot(w.T, col.loc[i_0_index:i_1_index])[0]\n",
    "        \n",
    "        df[i_1_index] = data\n",
    "                   \n",
    "    return df\n",
    "\n",
    "\n",
    "def MemoryVsCorr(total_range, series, treshold):\n",
    "    \n",
    "    interval = np.linspace(total_range[0], total_range[1], total_range[2])\n",
    "    result = pd.DataFrame(np.zeros((len(interval),4)))\n",
    "    result.columns = ['order','adf','corr', '5%']\n",
    "    result['order'] = interval\n",
    "\n",
    "    for counter,order in enumerate(interval):\n",
    "        seq_traf = transfer_data_by_frac_diff_FFD(series, order, treshold)\n",
    "        res=adfuller(seq_traf, maxlag=1, regression='c') #autolag='AIC'\n",
    "        result.loc[counter,'adf']=res[0]\n",
    "        result.loc[counter,'5%']=res[4]['5%']\n",
    "        seq_traf = seq_traf.values\n",
    "        difference = len(series) - len(seq_traf)\n",
    "        df1 = series[difference:].reset_index(drop=True).values\n",
    "        result.loc[counter, 'corr'] = np.corrcoef(df1, seq_traf)[0,1]\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def plotMemoryVsCorr(result, seriesName):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax2 = ax.twinx()  \n",
    "    color1='xkcd:deep red'; color2='xkcd:cornflower blue'\n",
    "    ax.plot(result.order,result['adf'],color=color1)\n",
    "    ax.plot(result.order, result['5%'], color='xkcd:slate')\n",
    "    ax2.plot(result.order,result['corr'], color=color2)\n",
    "    ax.set_xlabel('order of differencing')\n",
    "    ax.set_ylabel('adf', color=color1);ax.tick_params(axis='y', labelcolor=color1)\n",
    "    ax2.set_ylabel('corr', color=color2); ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    plt.title('ADF test statistics and correlation for %s' % (seriesName))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def df_after_frac(df_before, df_after):\n",
    "\n",
    "    difference = len(df_before) - len(df_after)\n",
    "    df_frac = df_after.to_frame().reset_index().copy()\n",
    "    df_frac.columns = [\"Date\", \"Diff\"]\n",
    "    df_frac.set_index(\"Date\", inplace = True)\n",
    "    \n",
    "    df_final = df_before[[\"Close\", \"Volume\", \"MACD\", \"Signal_Line\"]].merge(df_frac, on='Date', how='left')[difference:]\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_df)):\n",
    "    final_df[i].drop(\"label\", axis = 1, inplace = True)\n",
    "\n",
    "final_df[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "final_data = final_df.copy()\n",
    "\n",
    "stock_num = 0\n",
    "adf_list = []\n",
    "while stock_num < (len(stock_names)):\n",
    "    df_adf = pd.DataFrame(columns=[final_data[stock_num].columns])\n",
    "    print(stock_names[stock_num])\n",
    "    values_df = []\n",
    "    for col in final_data[stock_num].columns:\n",
    "        test_adf = MemoryVsCorr([0, 1, 21], final_data[stock_num][col], 1e-3)\n",
    "        for i in range(len(test_adf)):\n",
    "            if test_adf.loc[i][\"adf\"] < test_adf.loc[i][\"5%\"]:\n",
    "                values_df.append(test_adf.loc[i][\"order\"])\n",
    "                break\n",
    "            elif i == (len(test_adf)-1):\n",
    "                values_df.append(\"out of range\")\n",
    "            else:\n",
    "                pass\n",
    "    df_adf.loc[len(df_adf)] = values_df\n",
    "    adf_list.append(df_adf)\n",
    "    stock_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frac = []\n",
    "\n",
    "for i, data in enumerate(final_data):\n",
    "    df = pd.DataFrame(index = data.index)\n",
    "    for col in data.columns:\n",
    "        if adf_list[i].loc[0][col].item() != 0:\n",
    "            fractioned = transfer_data_by_frac_diff_FFD(data[col], adf_list[i].loc[0][col].item(),\n",
    "                                                        thres = 1e-3)\n",
    "            fractioned = fractioned.to_frame().reset_index()\n",
    "            fractioned.columns = [\"Date\", \"diff_{}\".format(col)]\n",
    "            fractioned.set_index(\"Date\", inplace=True)\n",
    "            df = df.join(fractioned, on = \"Date\")\n",
    "        else:\n",
    "            df = df.join(data[[col]])\n",
    "    df_frac.append(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.image import GramianAngularField\n",
    "import math\n",
    "\n",
    "def create_data(df, df_label, stock_names):\n",
    "    X_gasf_list = []\n",
    "    train_dax = []\n",
    "    test_dax = []\n",
    "    target_train = []\n",
    "    target_test = []\n",
    "    \n",
    "    df_scaled = ((df - df.max()) + (df - df.min()))/(df.max() - df.min())\n",
    "    \n",
    "    for i in range(30, len(df_scaled), 1):\n",
    "        X = df_scaled.iloc[i-30:i].T\n",
    "        gasf = GramianAngularField(method = \"summation\")\n",
    "        X_gasf = gasf.fit_transform(X)\n",
    "        X_gasf_list = [i for i in X_gasf]\n",
    "        dimensional = np.dstack(X_gasf_list)\n",
    "        \n",
    "        target = df_label[\"label\"].iloc[i-1]\n",
    "\n",
    "        if df.iloc[[i]].index < \"2019-01-01\":\n",
    "            train_dax.append(dimensional)\n",
    "            target_train.append(target)\n",
    "            \n",
    "        else:\n",
    "            test_dax.append(dimensional)\n",
    "            target_test.append(target)\n",
    "    \n",
    "    path_train = \"../eikon_api/{}/Train\".format(stock_names)\n",
    "    path_test = \"../eikon_api/{}/Test\".format(stock_names)\n",
    "    \n",
    "    os.makedirs(path_train)\n",
    "    os.makedirs(path_test)\n",
    "    \n",
    "    np.save(os.path.join(\"../eikon_api/{}\".format(stock_names), \"Train\", \"train.npy\"), train_dax)\n",
    "    np.save(os.path.join(\"../eikon_api/{}\".format(stock_names), \"Test\", \"test.npy\"), test_dax)\n",
    "\n",
    "    np.savetxt(os.path.join(\"../eikon_api/{}\".format(stock_names), \"Train\", \"target_train.csv\"), target_train)\n",
    "    np.savetxt(os.path.join(\"../eikon_api/{}\".format(stock_names), \"Test\", \"target_test.csv\"), target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = []\n",
    "\n",
    "for i in range(len(df_frac)):\n",
    "    result = pd.merge(df_frac[i], labels_df[i][\"label\"], on = \"Date\", how = \"left\")\n",
    "    final_df.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = []\n",
    "cnn_labels = []\n",
    "\n",
    "for i in range(len(final_df)):\n",
    "    cnn_labels.append(final_df[i][[\"label\"]])\n",
    "    data = final_df[i].drop(\"label\", axis = 1)\n",
    "    cnn_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cnn_data)):\n",
    "    create_data(cnn_data[i], cnn_labels[i], stock_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "train_label = []\n",
    "test = []\n",
    "test_label = []\n",
    "\n",
    "for i in range(len(stock_names)):\n",
    "    train.append(np.load(os.path.join(\"../eikon_api/{}\".format(stock_names[i]), \"Train\", \"train.npy\")))\n",
    "    train_label.append(pd.read_csv(\"../eikon_api/{}/Train/target_train.csv\".format(stock_names[i]), header = None))\n",
    "    test.append(np.load(os.path.join(\"../eikon_api/{}\".format(stock_names[i]), \"Test\", \"test.npy\")))\n",
    "    test_label.append(pd.read_csv(\"../eikon_api/{}/Test/target_test.csv\".format(stock_names[i]), header = None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(len(train[0][0,0,0,:])):\n",
    "    plt.subplot(6, 6, i+1)   \n",
    "    plt.axis('off')\n",
    "    plt.title(cnn_data[0].columns[i])\n",
    "    plt.imshow(train[0][0,:,:,i], cmap = \"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(len(train_label)):\n",
    "    y_train = train_label[i].iloc[:,0].values\n",
    "    y_test = test_label[i].iloc[:,0].values\n",
    "    y_train = np.where(y_train == -1, 0, y_train)\n",
    "    y_test = np.where(y_test == -1, 0, y_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_test_list[1], return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model_11(pool_type = \"max\", conv_activation = \"relu\", drop_out_conv = 0, drop_out_layer = 0.2):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), input_shape = (30,30,11), \n",
    "                                     activation = conv_activation))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = conv_activation))\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if drop_out_conv != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_conv))\n",
    "        \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = conv_activation))\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if drop_out_conv != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_conv))\n",
    "        \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = 256, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = 256, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = 2, activation =  \"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_10(pool_type = \"max\", conv_activation = \"relu\", drop_out_conv = 0, drop_out_layer = 0.2):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), input_shape = (30,30,10), \n",
    "                                     activation = conv_activation))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = conv_activation))\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if drop_out_conv != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_conv))\n",
    "        \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = conv_activation))\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if drop_out_conv != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_conv))\n",
    "        \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = 256, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = 256, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = 2, activation =  \"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_model_5(pool_type = \"max\", conv_activation = \"relu\", drop_out_conv = 0, drop_out_layer = 0.2):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), input_shape = (30,30,5), \n",
    "                                     activation = conv_activation))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = conv_activation))\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if drop_out_conv != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_conv))\n",
    "        \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = conv_activation))\n",
    "    if pool_type == \"max\":\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    if pool_type == \"average\":\n",
    "        model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    if drop_out_conv != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_conv))\n",
    "        \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = 256, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = 256, activation = conv_activation))\n",
    "    if drop_out_layer != 0:\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_layer))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(units = 2, activation =  \"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays as output otherwise will not work with sk-GS\n",
    "\n",
    "class BlockingTimeSeriesSplit():\n",
    "    \n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# feature importance \n",
    "btscv = BlockingTimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "feat_importance_df = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    print(stock_names[i])\n",
    "    selection_df = pd.DataFrame()\n",
    "    combination_list = []\n",
    "    score_list = []\n",
    "    y_train = y_train_list[i] \n",
    "    combinations = list(itertools.combinations(range(len(cnn_data[i].columns)), 5))\n",
    "    for j in range(len(combinations)+1):\n",
    "        if j == 0 and i == 0:\n",
    "            print(\"Base line\")\n",
    "            X_train = train[i].copy()\n",
    "            model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_model_11, epochs = 10, \n",
    "                                                                   batch_size = 16, verbose = 0)\n",
    "            combination_list.append([\"Base Line\"])\n",
    "        elif j == 0 and i == 1:\n",
    "            print(\"Base line\")\n",
    "            X_train = train[i].copy()\n",
    "            model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_model_10, epochs = 10, \n",
    "                                                                   batch_size = 16, verbose = 0)\n",
    "            combination_list.append([\"Base Line\"])\n",
    "        else:\n",
    "            combo_cols = [cnn_data[i].columns[idx] for idx in combinations[j-1]]\n",
    "            print(combo_cols)\n",
    "            train_input1 = train[i][:,:,:,combinations[j-1][0]].copy()\n",
    "            train_input2 = train[i][:,:,:,combinations[j-1][1]].copy()\n",
    "            train_input3 = train[i][:,:,:,combinations[j-1][2]].copy()\n",
    "            train_input4 = train[i][:,:,:,combinations[j-1][3]].copy()\n",
    "            train_input5 = train[i][:,:,:,combinations[j-1][4]].copy()\n",
    "            X_train = np.stack([train_input1, train_input2, train_input3, train_input4, train_input5], axis = 3)\n",
    "            model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_model_5, epochs = 10, \n",
    "                                                                   batch_size = 16, verbose = 0)\n",
    "            combination_list.append(combo_cols)\n",
    "            \n",
    "        tf.keras.backend.clear_session()\n",
    "        all_scores = cross_val_score(model, X_train, y_train, cv=btscv, scoring = \"accuracy\")\n",
    "        print(np.mean(all_scores))\n",
    "        score_list.append(np.mean(all_scores))\n",
    "    \n",
    "    selection_df[\"Combo\"] = combination_list\n",
    "    selection_df[\"Score\"] = score_list \n",
    "    feat_importance_df.append(selection_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "btscv = BlockingTimeSeriesSplit(n_splits=5)\n",
    "\n",
    "param_grid = {\n",
    "    \"pool_type\":[\"max\", \"average\"],\n",
    "    \"conv_activation\":[\"relu\", \"tanh\"],\n",
    "    \"drop_out_layer\":[0.1, 0.2],\n",
    "    \"batch_size\":[1, 16, 32, 64],\n",
    "    \"epochs\":[10, 20]\n",
    "}\n",
    "\n",
    "for i in range(len(selection_df)):\n",
    "    model_CV = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_model)\n",
    "    grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=btscv)\n",
    "    grid_result = grid.fit(X, y)\n",
    "\n",
    "    print(\"Best {}: {} using {}\".format(stock_names[i], grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_[\"mean_test_score\"]\n",
    "    stds = grid_result.cv_results_[\"std_test_score\"]\n",
    "    params = grid_result.cv_results_[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{} ({}) with: {}\".format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following (might be used later again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Close, Open (stock), Low (stock), High (stock) = Group 1\n",
    "- Volume, Turnover = Group 2\n",
    "- SMA 10, SMA 50, EMA 10, EMA 50, SMA 20, upper band, lower band, EMA 12, EMA 26, MACD, Signal Line = Group 3\n",
    "- f01 =\n",
    "- f02, f03, f04, f05, f06 = Group 2 (one removed)\n",
    "- f07\n",
    "- f08\n",
    "- f09\n",
    "- f10\n",
    "- Close (Gold) = Gold\n",
    "- High, Low, Open  = FX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select one variable from every group to overcome multicollinearity\n",
    "\n",
    "groups = [\"Base Line\", \"Group 1\", \"Group 1\", \"Group 1\", \"Group 1\", \"Group 2\", \"Group 2\", \"Group 3\", \"Group 3\", \n",
    "          \"Group 3\", \"Group 3\", \"Group 3\", \"Group 3\", \"Group 3\", \"Group 3\", \"Group 3\", \"Group 3\", \"Group 3\", \"X\",\n",
    "         \"Group 2\", \"Group 2\", \"Group 2\", \"Group 2\", \"Group 2\", \"X\", \"X\", \"X\", \"X\", \"Gold\", \"FX\", \"FX\", \"FX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_classification(data): \n",
    "    if data == 0:\n",
    "        return \"neutral\"\n",
    "    elif data > 0:\n",
    "        return \"negative influence\"\n",
    "    else:\n",
    "        return \"positive influence\"\n",
    "\n",
    "    \n",
    "feat_imp_results = []\n",
    "\n",
    "for i in range(len(feat_importance_df)):\n",
    "    \n",
    "    result = feat_importance_df[i].copy()\n",
    "    \n",
    "    result[\"Difference\"] = result - result.loc[\"total\"]\n",
    "    result = result[[stock_names[i], \"Difference\"]]\n",
    "    result[\"Classification\"] = result[\"Difference\"].apply(lambda x: feature_classification(x))\n",
    "    result[\"Group\"] = groups\n",
    "    idx_gb = result.groupby(\"Group\", as_index=False)\n",
    "    list_idx = [list(i) for i in idx_gb.groups.values()]\n",
    "    list_idx = [item for sublist in list_idx for item in sublist]\n",
    "    result = result.reindex(list_idx)\n",
    "    result = result.sort_values([\"Group\", stock_names[i], \"Classification\"])\n",
    "    feat_imp_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for i in range(len(feat_imp_results)):\n",
    "    for group in feat_imp_results[i][\"Group\"].unique():\n",
    "        display(feat_imp_results[i][feat_imp_results[i][\"Group\"] == group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = []\n",
    "\n",
    "for i in range(len(feat_imp_results)):\n",
    "    feat_df = feat_imp_results[i][(feat_imp_results[i][\"Classification\"] != \"negative influence\") &\n",
    "                                 (feat_imp_results[i][\"Classification\"] != \"neutral\")]\n",
    "    feat_group1 = feat_df[feat_df[\"Group\"] == \"Group 1\"].reset_index().copy()\n",
    "    feat_group1.set_index(\"index\", inplace = True)\n",
    "    feat_df = feat_df[feat_df[\"Group\"] != \"Group 1\"]\n",
    "    feat_df = feat_df.drop_duplicates(subset = [\"Group\"], keep = \"first\")\n",
    "    feat_df = feat_df.append(feat_group1.loc[[\"diff_Close\"]])\n",
    "    final_df.append(feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(final_df[1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_df)):\n",
    "    cols = [j for j in cnn_data[i].columns]\n",
    "    print(cols)\n",
    "    stock_idx = [j for j in range(len(cnn_data[i].columns)) if cols[j] in list(final_df[i].index)]\n",
    "    print(stock_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols)\n",
    "print(stock_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
